{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6462427d-f93a-46a3-aef1-f16e7a7f7bbd",
   "metadata": {},
   "source": [
    "# Questions on Naïve Bayes and k-Nearest Neighbors (kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0c2c3-3e22-4597-af3a-71beca5eede0",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee0b18-14aa-477b-bbf2-fb5c03b7abec",
   "metadata": {},
   "source": [
    "Assume that we want to use a Naïve Bayes classifier on a binary\n",
    "classification task, with the class labels being $c1$ and $c2$ and\n",
    "involving the binary features $f1$ and $f2$.  Moreover, asume a\n",
    "uniform class prior, i.e, $P(c1) = P(c2)$ and that the class\n",
    "conditional probabilities include $P(f1=0|c1) = 0$ and $P(f2=0|c2) = 1$.\n",
    "\n",
    "What class label $c \\in \\{c1,c2\\}$ maximizes $P(c|f1=0 \\& f2=1)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3f7d0-cae5-4ec6-8f3f-487203568c54",
   "metadata": {},
   "source": [
    "A: c1 will get a higher probability than c2\n",
    "\n",
    "B: c2 will get a higher probability than c1\n",
    "\n",
    "C: c1 and c2 will get the same probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e7087-3555-4536-b621-0515ed07b2cb",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd8e3c-8e2f-465d-b1f3-6d6bd466def0",
   "metadata": {},
   "source": [
    "Assume that we are facing a binary classification task, where a positive class label ($+$) is observed when the binary features $f_1$ and $f_2$ both have a value of 0 or 1, and a negative label ($-$) is observed in all other cases, i.e., when $f_1 \\neq f_2$. \n",
    "\n",
    "Can Naïve Bayes be expected to learn an accurate model for this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f08207-06e2-4dbf-aa35-3845df086260",
   "metadata": {},
   "source": [
    "A: Yes\n",
    "\n",
    "B: No\n",
    "\n",
    "C: Maybe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623cc6c-e3ed-432b-b6a8-43ece77d34c9",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730b20b-5366-4ba5-a5f4-82e6171522f0",
   "metadata": {},
   "source": [
    "Assume that a large number of binary features are added to a dataset\n",
    "with two class labels c1 and c2, such that for each added feature f,\n",
    "the class conditional probability $P(f=0|c1) = P(f=0|c2)$. What\n",
    "potential effect will the addition of such features have on the\n",
    "accuracy of Naïve Bayes and kNN respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab7f50-f79d-43aa-9906-960c4dbda555",
   "metadata": {},
   "source": [
    "A This will not have any effect on any of the algorithms\n",
    "\n",
    "B This will have an effect on Naïve Bayes only\n",
    "\n",
    "C This will have an effect on k-Nearest Neighbors only\n",
    "\n",
    "D This will have an effect on both algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee2962f-dab7-4ccc-a1cd-8d613af132a2",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d1224e-7e5d-4544-827d-f623354782cb",
   "metadata": {},
   "source": [
    "Assume that for one feature there is a large number of missing values, while the non-missing values are all identical. May the k-nearest neighbor algorithm using the Euclidean distance be affected if we chose to remove the feature completely instead of imputing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5522d4-1135-444a-8e9e-25870ad076f9",
   "metadata": {},
   "source": [
    "A: Yes and this holds independently of how we chose to do the imputation\n",
    "\n",
    "B: No, we will always get the same results\n",
    "\n",
    "C: The results will be the same if we chose to impute missing values with the mean of the non-missing\n",
    "\n",
    "D: The previous (C) would hold also even if the non-missing values are not identical "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2adaf2-2a35-4419-8be8-bceda2bfcc5d",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c75a9-712e-4d15-8b13-805e5ccca8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which of the following will be useful for finding the nearest neighbors \n",
    "# to a test object x_test in a set of instances X_train?\n",
    "\n",
    "# A\n",
    "A = sorted([distance(x_test,x_train) for x_train in X_train])\n",
    "\n",
    "# B\n",
    "B = sorted([(i, distance(x_test,x_train)) for i, x_train in enumerate(X_train)])\n",
    "\n",
    "# C\n",
    "C = sorted([(distance(x_test,x_train),i) for i, x_train in enumerate(X_train)])\n",
    "\n",
    "# D\n",
    "D = np.argsort([distance(x_test,x_train) for x_train in X_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c5d30-0007-4b65-a251-0f27ae52bcc3",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb1dbb-5f63-41be-bebe-0d8cb88740ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"CLASS\":[\"a\",\"b\",\"c\",\"c\",\"b\"], \n",
    "                   \"F1\":[1,1,2,2,3],\n",
    "                   \"F2\":[2,2,4,4,2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc13c55-b6b1-4efe-af78-80c32e393af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which of the following will generate a dictionary with a mapping from the\n",
    "# class labels to the relative frequencies?\n",
    "\n",
    "# A\n",
    "a = {}\n",
    "for c in df[\"CLASS\"].astype(\"category\").cat.categories:\n",
    "    a[c] = sum(df[\"CLASS\"]==c)/len(df)\n",
    "\n",
    "# B\n",
    "b = {}\n",
    "for c, g in df.groupby(\"CLASS\"):\n",
    "    b[c] = len(g)/len(df)\n",
    "\n",
    "# C\n",
    "c = {c:sum(df[\"CLASS\"]==c)/len(df) for c in pd.unique(df[\"CLASS\"])}\n",
    "\n",
    "# D\n",
    "d = df[\"CLASS\"].value_counts(normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
